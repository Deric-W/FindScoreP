@InProceedings{Score-P-Paper,
    author = "Kn{\"u}pfer, Andreas
    and R{\"o}ssel, Christian
    and Mey, Dieter an
    and Biersdorff, Scott
    and Diethelm, Kai
    and Eschweiler, Dominic
    and Geimer, Markus
    and Gerndt, Michael
    and Lorenz, Daniel
    and Malony, Allen
    and Nagel, Wolfgang E.
    and Oleynik, Yury
    and Philippen, Peter
    and Saviankou, Pavel
    and Schmidl, Dirk
    and Shende, Sameer
    and Tsch{\"u}ter, Ronny
    and Wagner, Michael
    and Wesarg, Bert
    and Wolf, Felix",
    editor="Brunst, Holger
    and M{\"u}ller, Matthias S.
    and Nagel, Wolfgang E.
    and Resch, Michael M.",
    title = "Score-P: A Joint Performance Measurement Run-Time Infrastructure for Periscope,Scalasca, TAU, and Vampir",
    booktitle = "Tools for High Performance Computing 2011",
    year = "2012",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "79--91",
    abstract = "This paper gives an overview about the Score-P performance measurement infrastructure which is being jointly developed by leading HPC performance tools groups. It motivates the advantages of the joint undertaking from both the developer and the user perspectives, and presents the design and components of the newly developed Score-P performance measurement infrastructure. Furthermore, it contains first evaluation results in comparison with existing performance tools and presents an outlook to the long-term cooperative development of the new system.",
    isbn = "978-3-642-31476-6"
}

@online{Score-P-Documentation,
    author = "{Score-P Contributors}",
    title = "Score-P Documentation",
    year = "2024",
    url = "https://perftools.pages.jsc.fz-juelich.de/cicd/scorep/tags/latest/html/index.html",
    urldate = {2024-08-05}
}

@online{mpi,
    author = "{Message Passing Interface Forum}",
    title = "{MPI}: A Message-Passing Interface Standard Version 4.1",
    url = "https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf",
    urldate = {2024-08-06},
    year = 2023,
    month  = nov
}

@online{openshmem,
    author = "Baker, Matthew
    and Boehm, Swen
    and Bouteiller, Aurelien
    and Chapman, Barbara
    and Cernohous, Bob
    and Culhane, James
    and Curtis, Tony
    and Dinan, James
    and Dubman, Mike
    and Goswami, Anshuman
    and Grodowitz, Megan
    and Grossman, Max
    and Hamidouche, Khaled
    and Hammond, Jeff
    and Itigin, Yossi
    and Lam, Bryant
    and Langer, Akhil
    and Linford, John
    and Manser, Jens
    and Mintz, Tiffany M.
    and Ozog, David
    and Park, Nicholas
    and Poole, Steve
    and Poole, Wendy
    and Pophale, Swaroop
    and Potluri, Sreeram
    and Pritchard, Howard
    and Md. Wasi-ur- Rahman
    and Ravichandrasekaran, Naveen
    and Raymond, Michael
    and Ross, James
    and Shamis, Pavel
    and Shende, Sameer
    and Si, Min
    and Venkata, Manjunath Gorentla
    ",
    title = "OpenSHMEM",
    year = "2020",
    month = "jun",
    url = "http://openshmem.org/site/sites/default/site_files/OpenSHMEM-1.5.pdf",
    urldate = {2024-08-06}
}

@online{openmp,
    author = "{OpenMP Architecture Review Board}",
    title = "OpenMP Application Programming Interface",
    year = "2018",
    month = "nov",
    url = "https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5.0.pdf",
    urldate = {2024-08-06}
}

@online{gomp,
    author = "{GCC team}",
    title = "GNU Offloading and Multi-Processing Project (GOMP)",
    url = "https://gcc.gnu.org/projects/gomp/",
    urldate = {2024-08-07}
}

@online{opencl,
    author = "{The Khronos Group Inc.}",
    title = "The OpenCL\texttrademark Specification",
    year = "2020",
    url = "https://registry.khronos.org/OpenCL/specs/3.0-unified/pdf/OpenCL_API.pdf",
    urldate = {2024-08-07}
}

@online{openacc,
    author = "{OpenACC-Standard.org.}",
    title = "The OpenACC\textregistered Application Programming Interface",
    year = "2022",
    url = "https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.3-final.pdf",
    urldate = {2024-08-07}
}

@online{nvidia-docs,
    author = "{NVIDIA Corporation \& affiliates}",
    title = "NVIDIA Documentation Hub",
    url = "https://docs.nvidia.com/",
    urldate = {2024-08-08}
}

@online{rocm-docs,
    author = "{Advanced Micro Devices, Inc}",
    title = "AMD ROCm documentation",
    url = "https://rocm.docs.amd.com/en/latest/",
    urldate = {2024-08-08}
}

@online{CMake-Documentation,
    author = "{Kitware, Inc. and Contributors}",
    title = "CMake Reference Documentation",
    url = "https://cmake.org/cmake/help/v3.20/index.html",
    urldate = {2024-08-05}
}

@article{gromacs,
    title = {GROMACS: A message-passing parallel molecular dynamics implementation},
    journal = {Computer Physics Communications},
    volume = {91},
    number = {1},
    pages = {43-56},
    year = {1995},
    issn = {0010-4655},
    doi = {https://doi.org/10.1016/0010-4655(95)00042-E},
    url = {https://www.sciencedirect.com/science/article/pii/001046559500042E},
    author = {H.J.C. Berendsen and D. {van der Spoel} and R. {van Drunen}},
    keywords = {Molecular dynamics, Parallel computing},
    abstract = {A parallel message-passing implementation of a molecular dynamics (MD) program that is useful for bio(macro)molecules in aqueous environment is described. The software has been developed for a custom-designed 32-processor ring GROMACS (GROningen MAchine for Chemical Simulation) with communication to and from left and right neighbours, but can run on any parallel system onto which a a ring of processors can be mapped and which supports PVM-like block send and receive calls. The GROMACS software consists of a preprocessor, a parallel MD and energy minimization program that can use an arbitrary number of processors (including one), an optional monitor, and several analysis tools. The programs are written in ANSI C and available by ftp (information: gromacs@chem.rug.nl). The functionality is based on the GROMOS (GROningen MOlecular Simulation) package (van Gunsteren and Berendsen, 1987; BIOMOS B.V., Nijenborgh 4, 9747 AG Groningen). Conversion programs between GROMOS and GROMACS formats are included. The MD program can handle rectangular periodic boundary conditions with temperature and pressure scaling. The interactions that can be handled without modification are variable non-bonded pair interactions with Coulomb and Lennard-Jones or Buckingham potentials, using a twin-range cut-off based on charge groups, and fixed bonded interactions of either harmonic or constraint type for bonds and bond angles and either periodic or cosine power series interactions for dihedral angles. Special forces can be added to groups of particles (for non-equilibrium dynamics or for position restraining) or between particles (for distance restraints). The parallelism is based on particle decomposition. Interprocessor communication is largely limited to position and force distribution over the ring once per time step.}
}

@online{gromacs-git,
    author = "{The GROMACS Crew}",
    title = "GROMACS - Gitlab",
    url = "https://gitlab.com/gromacs/gromacs",
    urldate = {2024-08-20}
}